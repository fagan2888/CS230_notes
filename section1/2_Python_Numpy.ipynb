{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"darkblue\">Python Basics with Numpy</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Set test to `\"Hello World\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = \"Hello, World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"darkblue\">Building basic functions with Numpy</font>\n",
    "\n",
    "Numpy is the main package for scientific computing in Python. \n",
    "\n",
    "### Sigmoid function\n",
    "\n",
    "Before using `np.exp()`, you will use `math.exp()` to see why `np.exp()` is preferable.\n",
    "\n",
    "**Exercise**: Build a function that returns the sigmoid of a real number `x`. Use `math.exp(x)`\n",
    "\n",
    "**Reminder**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output \n",
    "`basic_sigmoid(3) = 0.9525741268224334`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rarely use the `math` library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why `Numpy` is more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning\n",
    "# x = [1, 2, 3]\n",
    "# basic_sigmoid(x) \n",
    "# You will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.71828183   7.3890561   20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3])       # array op\n",
    "print(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [4 5 6]\n",
      "Y = [ 1.          0.5         0.33333333]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])       # vector op\n",
    "print(\"X = \" + str(x + 3))\n",
    "print(\"Y = \" + str(1 / x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement `sigmoid` function using numpy. \n",
    "\n",
    "**Instructions**: Input could now be a real number, vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices...) are called numpy arrays.\n",
    "\n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } \\sigma(x) = \\sigma\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73105858,  0.88079708,  0.95257413])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output \n",
    "`sigmoid([1,2,3]) : array([ 0.73105858,  0.88079708,  0.95257413])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid gradient\n",
    "\n",
    "**Exercise**: Implement the function `sigmoid_grad()` to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 -s)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [ 0.19661193  0.10499359  0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** : `sigmoid_derivative([1,2,3]) = [ 0.19661193  0.10499359  0.04517666]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping arrays\n",
    "\n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape is used to reshape X into some other dimension. \n",
    "\n",
    "For example, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "\n",
    "**Exercise**: Implement generic `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length $\\times$ height $\\times$ 3, 1). \n",
    "\n",
    "For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) \n",
    "# v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image2vector FUNCTION\n",
    "\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length * height * depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2], 1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = \n",
      "[[ 0.67826139]\n",
      " [ 0.29380381]\n",
      " [ 0.90714982]\n",
      " [ 0.52835647]\n",
      " [ 0.4215251 ]\n",
      " [ 0.45017551]\n",
      " [ 0.92814219]\n",
      " [ 0.96677647]\n",
      " [ 0.85304703]\n",
      " [ 0.52351845]\n",
      " [ 0.19981397]\n",
      " [ 0.27417313]\n",
      " [ 0.60659855]\n",
      " [ 0.00533165]\n",
      " [ 0.10820313]\n",
      " [ 0.49978937]\n",
      " [ 0.34144279]\n",
      " [ 0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array\n",
    "\n",
    "image = np.array([\n",
    "        [[ 0.67826139,  0.29380381], [ 0.90714982,  0.52835647], [ 0.4215251 ,  0.45017551]],\n",
    "        [[ 0.92814219,  0.96677647], [ 0.85304703,  0.52351845], [ 0.19981397,  0.27417313]],\n",
    "        [[ 0.60659855,  0.00533165], [ 0.10820313,  0.49978937], [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \\n\" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing rows\n",
    "\n",
    "Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. \n",
    "\n",
    "Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For e.g., if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix} $$and        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "\n",
    "**Exercise**: Implement `normalizeRows()` to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalizeRows\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_norm = np.linalg.norm(x, axis = 1, keepdims = True)\n",
    "    x = x / x_norm\n",
    "    print(\"shape of x_vect =\" + str(x.shape))\n",
    "    print(\"shape of x_norm =\" + str(x_norm.shape))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_vect =(2, 3)\n",
      "shape of x_norm =(2, 1)\n",
      "normalizeRows(x) = \n",
      "[[ 0.          0.6         0.8       ]\n",
      " [ 0.13736056  0.82416338  0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \\n\" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting and the softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement a `softmax` function. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes.\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{for } X \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(X) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{for a matrix } X \\in \\mathbb{R}^{m \\times n} \\text{,}$  \n",
    "$$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    softmax = x_exp/x_sum\n",
    "\n",
    "    print(\"dimension of input:   \" + str(x.shape))\n",
    "    print(\"dimension of row_sum: \" + str(x_sum.shape))\n",
    "    print(\"dimension of softmax: \" + str(softmax.shape))  \n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of input:   (2, 4)\n",
      "dimension of row_sum: (2, 1)\n",
      "dimension of softmax: (2, 4)\n",
      "softmax(x) = \n",
      "[[  9.81016419e-01   8.94571181e-04   1.79679425e-02   1.21067044e-04]\n",
      " [  8.79384465e-01   1.19011746e-01   8.01894834e-04   8.01894834e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0],\n",
    "    [7, 5, 0, 0]])\n",
    "print(\"softmax(x) = \\n\" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you print the shapes of x_exp, x_sum and softmax above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and softmax are of shape (2,4). This is due to Numpy broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Vectorization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5 5 2 3 2 7 0 2 7 5 8 0 8 2 8 7 1 5 8 4 4 6 1 4 1 6 6 7 8 5 0 1 4 6 8 0\n",
      " 4 0 5 8 1 6 8 3 5 6 2 5 2 1 7 3 5 0 7 7 4 7 3 5 0 7 2 4 7 4 4 8 3 8 1 8 0\n",
      " 4 2 8 7 1 4 6 4 7 2 8 5 0 6 2 0 7 3 8 2 3 4 7 3 4 2]\n",
      "[3 0 6 7 4 3 0 0 7 2 1 4 3 0 4 6 1 8 5 5 3 8 5 1 3 5 1 2 8 1 2 7 5 6 5 7 4\n",
      " 4 6 7 3 4 3 2 4 6 6 3 3 4 6 8 2 7 7 2 1 2 8 7 0 5 6 8 5 6 8 6 4 6 3 0 3 5\n",
      " 5 5 5 6 8 2 1 7 0 7 7 7 5 6 4 1 8 4 7 8 2 5 0 7 5 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 = np.random.randint(low = 0, high = 9, size = 100)\n",
    "x2 = np.random.randint(low = 0, high = 9, size = 100)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product        1801\n",
      "Computation time = 0.12487299999996981ms\n",
      "\n",
      "\n",
      "Outer product    =\n",
      "[[ 18.   0.  36. ...,  42.  30.   0.]\n",
      " [ 15.   0.  30. ...,  35.  25.   0.]\n",
      " [ 15.   0.  30. ...,  35.  25.   0.]\n",
      " ..., \n",
      " [  9.   0.  18. ...,  21.  15.   0.]\n",
      " [ 12.   0.  24. ...,  28.  20.   0.]\n",
      " [  6.   0.  12. ...,  14.  10.   0.]]\n",
      "Computation time = 4.755086999999936ms\n",
      "\n",
      "\n",
      "Elementwise multiplication = \n",
      "[ 18.   0.  30.  14.  12.   6.   0.   0.  14.  14.   5.  32.   0.   0.   8.\n",
      "  48.   7.   8.  25.  40.  12.  32.  30.   1.  12.   5.   6.  12.  56.   8.\n",
      "  10.   0.   5.  24.  30.  56.   0.  16.   0.  35.  24.   4.  18.  16.  12.\n",
      "  30.  36.   6.  15.   8.   6.  56.   6.  35.   0.  14.   7.   8.  56.  21.\n",
      "   0.   0.  42.  16.  20.  42.  32.  24.  32.  18.  24.   0.  24.   0.  20.\n",
      "  10.  40.  42.   8.   8.   6.  28.   0.  14.  56.  35.   0.  36.   8.   0.\n",
      "  56.  12.  56.  16.   6.  20.   0.  21.  20.   0.]\n",
      "Computation time = 0.16686300000001708ms\n",
      "\n",
      "\n",
      "general dot prod = [ 226.1020921   219.59727277  200.11659373]\n",
      "Computation time = 0.368322000000032ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "### CLASSIC DOT PRODUCT \n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot product        \" + str(dot))\n",
    "print (\"Computation time = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2))) # Create a len(x1)*len(x2) zero matrix\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"Outer product    =\\n\" + str(outer))\n",
    "print (\"Computation time = \"  + str(1000*(toc - tic)) + \"ms\\n\\n\")\n",
    "\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "assert(len(x1) == len(x2))\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"Elementwise multiplication = \\n\" + str(mul))\n",
    "print (\"Computation time = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")\n",
    "\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"general dot prod = \" + str(gdot))\n",
    "print (\"Computation time = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product      = 1801\n",
      "Computation time = 0.08040600000003284ms\n",
      "\n",
      "\n",
      "outer = \n",
      "[[18  0 36 ..., 42 30  0]\n",
      " [15  0 30 ..., 35 25  0]\n",
      " [15  0 30 ..., 35 25  0]\n",
      " ..., \n",
      " [ 9  0 18 ..., 21 15  0]\n",
      " [12  0 24 ..., 28 20  0]\n",
      " [ 6  0 12 ..., 14 10  0]]\n",
      "Computation time = 0.5502809999999858ms\n",
      "\n",
      "\n",
      "Elementwise multiplication = \n",
      "[18  0 30 14 12  6  0  0 14 14  5 32  0  0  8 48  7  8 25 40 12 32 30  1 12\n",
      "  5  6 12 56  8 10  0  5 24 30 56  0 16  0 35 24  4 18 16 12 30 36  6 15  8\n",
      "  6 56  6 35  0 14  7  8 56 21  0  0 42 16 20 42 32 24 32 18 24  0 24  0 20\n",
      " 10 40 42  8  8  6 28  0 14 56 35  0 36  8  0 56 12 56 16  6 20  0 21 20  0]\n",
      "Computation time = 0.050516999999916656ms\n",
      "\n",
      "\n",
      "general dot product = [ 226.1020921   219.59727277  200.11659373]\n",
      "Computation time    = 2.718489000000046ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### VECTORIZED DOT PRODUCT\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot product      = \" + str(dot))\n",
    "print (\"Computation time = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \\n\" + str(outer))\n",
    "print (\"Computation time = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"Elementwise multiplication = \\n\" + str(mul))\n",
    "print (\"Computation time = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"general dot product = \" + str(dot))\n",
    "print (\"Computation time    = \" + str(1000*(toc - tic)) + \"ms\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>L1 and L2 loss functions</font>\n",
    "\n",
    "**Exercise**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Reminder**:\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$).\n",
    "\n",
    "- L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = np.sum(np.abs(yhat - y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y    = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output** `L1 = 1.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the L2 loss. \n",
    "\n",
    "**Reminder** \n",
    "- if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    diff = y - yhat  \n",
    "    loss = np.sum(np.dot(diff,diff)) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: `L2 = 0.43`"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
